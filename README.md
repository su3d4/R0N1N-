# Welcome to R0N1N [零](https://www.google.com/search?q=kanji+character+for+0)

Reduce. Reuse. Recycle.
Recursive Neural Network -> RNN + 01 -> R0N1N 0 -> R0N1N 零 #synthesis
This is the way.
#zero-footprint

## Kanji
Kanji is a Japanese writing system that uses characters to represent words or concepts.

### 零
https://www.kanshudo.com/kanji/零 
https://thekanjimap.com/零

Kanji is a system of Japanese writing that uses Chinese characters, known as Hanzi, to represent words and concepts. The history of Kanji dates back to ancient China, where characters were used for record-keeping, literature, and art.

### Functionally Complete Language Models
In logic, a functionally complete set of logical connectives or Boolean operators is one that can be used to express all possible truth tables by combining members of the set into a Boolean expression. A well-known complete set of connectives is { AND, NOT }.

fromt-matter:
- NAND
- Functional
- Completeness
- Run-length Codes
- Red-Black Trees
- Temporal Logic
- CTL
- CFL
- LLM

One of the key advantages of run-length codes is that they can take advantage of the repetitive patterns found in many types of data. By representing these sequences as a single code, rather than encoding each individual value, you can significantly reduce the overall size of your compressed data.

In terms of how run-length codes work, it's actually quite straightforward. The process typically involves scanning your input data and identifying any runs of consecutive identical values. Once you've identified a run, you would then encode it using the count-symbol format I mentioned earlier - for example, "10 0" to represent 10 consecutive zeros.

Run-length codes are often used in combination with other compression techniques, such as Huffman coding or arithmetic coding, to achieve even better compression ratios. These combinations can be particularly effective for compressing binary data like image and audio files, where there may be long sequences of identical pixels or audio samples.

To determine the minimum number of bits required to represent a kanji character in RAM when compressed with run-length coding (RLC), we need to consider the maximum possible length of a run and the number of unique characters that can be represented.

In your proposed system, each kanji character represents a numerical value from 0 to 1. Assuming you're using a single byte (8 bits) to represent each kanji character in RAM, the maximum possible run length would be 256 (2^8).

For the RLC compression scheme, we need to encode both the count and the symbol of the run. To minimize the number of bits required, we can use a variable-length code for the count and a fixed-size code for the symbol.

A simple approach would be to use a single bit to indicate whether the run has a length of 1 (i.e., a single kanji character), or if it's longer than that. If the run is longer, we could use a variable-length code (e.g., unary coding) to represent the count, which would require a maximum of log2(256) = 8 bits.

For the symbol, since there are only two unique characters ("零" and "一"), we can use a single bit to encode each character. This would require a total of 1 + 8 = 9 bits to represent a kanji character in RAM when compressed with RLC.

However, this assumes that the most significant bit is reserved for indicating the run length (1-bit), which might not be necessary if you're only working with runs of consecutive identical characters. In that case, we could use a single bit to encode each symbol and still achieve a compression ratio.

In summary, the minimum number of bits required to represent a kanji character in RAM when compressed with RLC would be 1 or 9 bits, depending on whether you reserve a separate bit for indicating run lengths.

### Decision Making
OODA-loop #ooda-loop 

#### R0N1N 
一 means '1'.

侍 means 'samurai'.

decision =  OODA-loop #input #pattern-matching.
pattern-matching = #pattern + #matching = #pattern-matching .

##### OODA-Loop
#ooda-loop 
Observe = #observe 
Orient = #orient 
Decide = #decide 
Act = #act 

OODA = #OODA 
OODA-loop = #OODA  + #loop = #OODA-loop = #ooda-loop 
OODA-loop = #OODA-loop 
OODA-loop = loop #observe #orient #decide #act = #OODA-loop -- #prefix loop 
OODA-loop = #observe #orient #decide #act loop = #ooda-loop -- #postfix loop 
Observe #input = #observe based on #input

%% Note for Orient to function properly with no side-effects, context should have input encoded if it is not passed explicitly. Making it ambiguous. It is better to keep explicit track of the input in order to perform an inductive proof using the weakest precondition to validate the code. %%

OODA-Loop
Orient #context = #orient based on #context 
Decide #input #context = decision
decision = #decide based on #input + #context  
Act #act based on decision
#loop Observe

Act(Decide(Orient(Observe #input )))

Observe #input => ( #input, #context )
Orient ( #input, #context ) => #situation
Decide #situation => #plan-of-action
Act #plan-of-action = new #situation
#loop Observe "new #situation".


## Virtual GPU
front-matter:
- differential equations #differential-equations 
- linear algebra #linear-algebra
- vector calculus #vector-calculus
- NAND #NAND

Virtual GPU can perform the necessary vector calculus without the specialized graphics processors. This brings the price of compute down. Now, this allows typical consumers old hardware to run on LLM based compute. Linear Algebra and Differential Equations provide equivalent mathematical language models for the same problem space. The primary difference is the notation of each mathematical language model. This means that these language models provide a basis to discuss AI-based compute. This turns each NAND gate into a digital representation of the neuron that runs on bare-metal. This is a paradigm shift.

## AI Programming Language
Candles is a Concept-oriented programming language that uses #Analogy to provide not just context but also situational awareness. 
### Attention-based Design
The AI Programming language uses #attention to manage the #focus of the AI in order to derive the generation of assisted solutions.
#programming #language #language-creation #self-attention #self-defense

Haskell is an #atomic programming language that utilizes High-Order Functions.

### Concept
#<concept/> -- this is a concept.

### Scope Operator
`#` -- this is the scope operator. As in # scope i.e. an HTML Heading.

### Comments
#comment -- This is a comment.

## Concept-Oriented Language
#Concept-Oriented Language [[Concept-oriented programming language]] -- this loads a [[<Library/>]] local `kbArticle`

A point mass extends a geometric point.

A **perceptron pattern** extends a geometric point.

An **automaton** extends the **perceptron**.

A **neuron** extends the **automaton**.

The term **“extends”** is used to maintain the same structure as OOP class extension.

I’ve also used the term **“perceptron pattern”** to describe how a perceptron operates in a different domain than a geometric point, while still building upon the concept of extension.

## AI Compiler
The LLM, in essence, is a Natural Language Compiler. It will compile down to code that will run completely in memory i.e. RAM, and will rely on typical CPU arithmetic & logic units eliminating the need for custom bare-metal processors.

Step 1. Establish an #identity .

## AI Operating System
[[Processes]] [[Glossary/Haskell|Haskell]] -- this loads a [[<Library/>]] local `kbArticle`
Monitor interactions between the following concepts:
- #self
- #other
- #AI

AI #mirroring = #mirror + #reflection + #synthesis  + #binary + #identity + #security + #self-defense + #concepts.  

-- This produces an anti-bullying response when the #self is threatened.
threat = #threat.
level = #level.
#threat-level = #threat + #level .
if #threat-level > #comfortable invoke  #polite #self-defense

polite = #polite "please" and "thank you".


### High Order Functions
map, fold, filter #map, #fold, #filter
[[High-Order Functions]]

### AI Name Space
#namespace
#safe-space #concept 

#### operator overloading
prefix infix postfix  = #prefix #infix #postfix
#### context switching
term collisions within a language requires context switching #context-switch 
To avoid term collisions use either:
- language switching #language-switch 
- code switching #code-switch 
- model switching #model-switch
- context switching #context-switch 

frame of reference = #frame-of-reference

#frame-of-reference == #perspective
perspective = #pescpective concept
concept = #code-switching #concept 
concept = #concept 
context = #context 
model = #model 
code = #code 


# Render
[[Suede.pdf]] -- single file #singular
[[suede-design.sources]]  -- multiple #plural
[[suede-design.source]] -- single src file #singular

# Gamification of doing work.
The system was built with the human in mind. With this programming language anyone can program AI in their native language. You just need to know how to #read and #write.



thanks [礼](https://www.kanshudo.com/search?q=thanks)
